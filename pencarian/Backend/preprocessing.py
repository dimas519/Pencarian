# -*- coding: utf-8 -*-
"""preprocessing

Automatically generated by Colaboratory.

@author: B. Lukas

Original file is located at
    https://colab.research.google.com/drive/1GPuAqWPMJdaivtPYAcIMKdVDG6ILAeCi

PREPROCESSING
"""

# fungsi" preprocess
# tiap fungsi inputnya 1 kalimat dalam konteks yang sama (bisa 1 kalimat / lebih / 1 paragraf) >> 1 string

import re 
import simplemma
from nltk import word_tokenize, pos_tag, sent_tokenize
from nltk.corpus import stopwords

import spacy

def cleaner(txt):
    # menghapus seluruh symbol, kecuali huruf dan angka
    txt = re.sub('[^A-Za-z0-9\' ]', '', str(txt))
    return txt 

def spaceInsert(txt):
    # adding space before special char
    txt = re.sub('([^A-Za-z0-9 ])', ' \\1', str(txt))
    return txt 

def tokenizer(df):
    tokenized = word_tokenize(df.lower())
    return tokenized

def removeStopWord(txt):
    # pilih stop word dengan bahasa apa yang ingin di hilangkan
    stopWord = stopwords.words('english')
    tokens_wo_stopwords = [t for t in tokenizer(txt) if t not in stopWord]
    # print(tokens_wo_stopwords)

"""POS Tagger"""

def posTag(txt):
  resList = sent_tokenize(txt)
  txtRes = []
  for stc in resList:
        x = re.sub('[^A-Za-z0-9 ]','', stc)
        txtRes.append(x)
  tokenizedSentencesRes = [word_tokenize(sentence) for sentence in txtRes] 
  #menambahkan fitur POS tag
  taggedSentencesRes = [pos_tag(stc, tagset = 'universal') for stc in tokenizedSentencesRes]
  return taggedSentencesRes

def POScomposition(taggedSentences):
    #membuat dictionary untuk menyimpan jumlah setiap tag dalam text
    corpus = {} 
    for sentence in taggedSentences:
        for a,b in sentence:
            if b in corpus.keys():
                corpus[b] += 1
            else:
                corpus[b] = 1
                
    #menghitung total tag yang ada pada text
    count = 0
    for i in corpus.values():
        count += i
        
    #dictionary untuk menyimpan analisa penghitungan setiap tag
    dicti = {}
    for sentence in taggedSentences:
        for a,b in sentence:
            dicti[b] = corpus[b]/count
            
    return dicti

def lemmaSpacy(txt):
    nlp = spacy.load('en_core_web_sm')
 
    # Create a Doc object
    doc = nlp(txt)

    lemmatized = " ".join([token.lemma_ for token in doc])
    
    return lemmatized

def simpleLemma(txt):
    langdata = simplemma.load_data('en')
    
    lemmatized = " ".join([simplemma.lemmatize(token, langdata) for token in txt.split()])
    
    return lemmatized